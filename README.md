# CLI-клиент для LLM

Простой CLI-скрипт для отправки запросов в LLM через LiteLLM Proxy.

## Установка

1. Установите зависимости:
```bash
pip3 install -r requirements.txt
```

## Запуск и тестирование

### 1. Установка зависимостей
```bash
cd /Users/annacudinova/TestCline
pip3 install -r requirements.txt
```

### 2. Запуск скрипта
```bash
python3 llm_client.py
```

При первом запуске скрипт запросит параметры по очереди:
```
Введите OPENAI_BASE_URL: (ваш URL)
Введите OPENAI_MODEL: (название модели)
Введите OPENAI_API_KEY: (ваш ключ)
```
После этого появится диалоговое окно для ввода сообщений.

### 3. Тестирование
После ввода параметров введите сообщение:
```
Вы: напиши 3 предложения о котиках
```

Для выхода введите `exit`.

**Примечание:** Ошибка 401 означает неверный API-ключ. Ключ должен начинаться с `sk-`.

### Альтернатива: через переменные окружения
Задайте переменные перед запуском (одной строкой):
```bash
export OPENAI_BASE_URL="ваш_url" OPENAI_MODEL="название_модели" OPENAI_API_KEY="ваш_ключ" && python3 llm_client.py
```

Или последовательно:
```bash
export OPENAI_BASE_URL="ваш_url"
export OPENAI_MODEL="название_модели"
export OPENAI_API_KEY="ваш_ключ"
python3 llm_client.py
```

## Пример

```
==================================================
CLI-клиент для LLM
==================================================
Введите ваше сообщение (или 'exit' для выхода)
--------------------------------------------------

Вы: Привет, напиши короткое стихотворение

LLM: Привет! Вот короткое стихотворение для вас:
...
